# Schematic Hardware Design of GPUs

A modern GPU contains a set of multi-threaded streaming multiprocessors (SM), which are independent execution units. 
Each SM contains 

  - An array of scalar processors
  - Registers
  - High-bandwidth low-latency shared memory
  - Cache for constant and texture memory
  
In addition a GPU has a large amount of global device memory, which is accessible from the host CPU.

<img src="../content/images/gpuModel.jpg" width="650" alt="GPU Model">

A CPU program invokes a parallel program on the GPU in many threads, which are grouped into blocks of threads. 
The blocks are scheduled on SMs with free capacity. All the threads of a thread block execute concurrently on an SM.
Multiple thread blocks can execute concurrently on a single SM. As thread blocks terminate, new blocks are launched.

A streaming multiprocessor is designed to execute hundreds of threads concurrently on its array of scalar processors. To manage such a large amount 
of threads, it employs a so called single instruction multiple thread (SIMT) architecture, which schedules threads not one by one but in groups of 
32 threads called warps. The SM creates, manages, schedules and executes warps. Individual threads composing a warp start together at the same program 
address, but they have their own instruction address counter and register state and are therefore can branch and execute independently. 
When a multiprocessor is given one or more thread blocks to execute, it partitions them into warps and each warp gets scheduled for execution. 
Blocks are partitioned into warps of threads with consecutive, increasing thread IDs with the first warp containing thread ID 0. 

On each cycle, a warp executes one common instruction. Full efficiency is only realized when all 32 threads of a warp have the same execution path. 
If threads of a warp diverge because of data-dependent conditional branches, the warp serially executes each branch path taken, disabling threads 
that are not on that path, and when all paths complete, the threads converge back to the same execution path. Branch divergence occurs only within a warp. 
Different warps execute independently regardless of whether they are executing common or disjoint code paths.

The execution context of each warp processed by a SM is maintained on-chip during the entire lifetime of the warp. Therefore, switching from one 
execution context to another has no cost. At every instruction issue time the warp scheduler selects a warp which has all threads ready to execute 
its next instruction and issues the instruction to those threads. 

Each SM has a set of registers that are partitioned among the warps, and shared memory that is partitioned among the thread blocks. The number of 
blocks and warps that can reside and be processed together on a SM for a given kernel depends on the resource requirements of the kernel in terms 
of registers, shared memory and local memory, and also on the the available resources on the SM. There are also a maximum number of resident 
blocks and a maximum number of resident warps per SM. These limits as well as the amount of registers and shared memory available on the SM 
depend on the compute capability of the device. If there are not enough registers or shared memory available per SM to process at least one 
single block, the kernel will fail to launch.

The actual number of blocks and warps residing on each SM for a given kernel call depends on the execution configuration of the call, 
the resource requirements of the kernel, and the available resources of the SM. The occupancy of a kernel is an important performance 
characteristics of a kernel. It is defined as the ratio of the number of resident warps to the maximum number of warps per SM. 
Whenever possible, the programmer should aim to design the kernel for 100% occupancy.  