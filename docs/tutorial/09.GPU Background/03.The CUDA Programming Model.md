# The CUDA Programming Model

In order to develop software applications on GPUs, NVIDIA introduced the CUDA parallel programming model in late 2006. 
CUDA is a trademark of NVIDIA. A detailed documentation can be found [online](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html).

The design goal of CUDA is that applications can transparently scale their parallelism to leverage from increasing number of processor cores on future hardware devices. CUDA introduces three key abstractions

  - A hierarchy of threads, grouped into block of threads and a grid of thread blocks
  - Shared memory 
  - Barrier synchronization

These abstractions provide two nested levels of parallelism

  - Fine-grained data parallelism and thread parallelism for threads of a block
  - Coarse-grained data parallelism and task parallelism for block of threads

The two levels of parallelism allow to partition the problem into coarse sub-problems that can be solved independently in parallel by blocks of threads, and each sub-problem into smaller pieces that can be solved cooperatively in parallel by all threads within the block, exploiting fine-grained parallelism, data exchange and synchronization. 

Such a programming model enables scalability: each block of threads can be scheduled on any of the available multiprocessors within a GPU, in any order, concurrently or sequentially, so that a CUDA program can execute on any number of multiprocessors.

In the CUDA parallel programming model, an application consists of a sequential host program and multiple parallel programs called kernels. A kernel is a single program multiple data (SPMD) computation that is executed within a potentially large number of threads in parallel.

Each thread that executes a kernel is identified through a unique thread index. Similarly, the thread block of the thread has a unique block index within the grid of blocks. A kernel can interrogate its thread and block indices through the built in variables threadIdx and blockIdx. The dimension of the  block and the grid can be queried from the variable blockDim, respectively `gridDim`.

<table border="1">
<tr><th>Built in Variable</th><th>Description</th></tr>
<tr><td>threadIdx (x,y,z)</td><td>thread index within the thread block</td></tr>
<tr><td>blockDim (x,y,z)</td><td> of the thread block</td></tr>
<tr><td>blockIdx (x,y,z)</td><td>block index within the grid </td></tr>
<tr><td>gridDim (x,y,z)</td><td> of the grid</td></tr>
<tr><td>warpSize </td><td>warp size in threads of the current architecture</td></tr>
</table>

The three dimensional index scheme provides more flexibility to map a computational problem to a collection of parallel threads.

A kernel can be executed by multiple thread blocks of identical dimension. Consequently the total number of threads is the number of blocks times the number of threads per block. Note that there is a limit to the number of threads per block, since all threads of a block are expected to reside on the same SM and must share its limited  resources. On current GPUs, the size of a thread block can be as large as 1024 threads. 

Thread blocks are required to execute independently of each other, without any  given ordering, sequentially or in parallel. On the other hand, threads within a block can cooperate by sharing data through shared memory and can be synchronized. The intrinsic function `__syncthreads()` specifies synchronization points, which act as barriers at which all threads in the block must wait before any is allowed to proceed.

During execution, threads may access data from multiple memory spaces. Each thread has private local memory. Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. All threads have access to the same global memory. 
Constant and texture memory are two additional read-only memory spaces accessible by all threads. The global, constant, and texture memory 
is optimized for different memory usages. Texture memory has additional addressing modes and provides data filtering and interpolation. 
Global, constant, and texture memory is persistent across kernel launches.

<table border="1">
<tr><th>Memory</th><th>Location</th><th>Cached</th><th>Access</th><th>Scope</th><th>Lifetime</th></tr>
<tr><td>Register</td><td>on chip</td><td>n/a</td><td>r/w</td><td>1 thread</td><td>thread</td><td>
<tr><td>Local</td><td>off chip</td><td>yes (sm20)</td><td>r/w</td><td>1 thread</td><td>thread</td><td>
<tr><td>Shared</td><td>on chip</td><td>n/a</td><td>r/w</td><td>all threads in block</td><td>block</td><td>
<tr><td>Device</td><td>off chip</td><td>yes</td><td>r/w</td><td>all threads and host</td><td>host allocation</td><td>
<tr><td>Constant</td><td>off chip</td><td>yes</td><td>r</td><td>all threads and host</td><td>host allocation</td><td>
<tr><td>Texture</td><td>off chip</td><td>yes</td><td>r</td><td>all threads and host</td><td>host allocation</td><td>
</table>


