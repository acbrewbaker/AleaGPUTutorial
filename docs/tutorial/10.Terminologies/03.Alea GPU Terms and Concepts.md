# Alea GPU Terms and Concepts 

Alea GPU introduces the following key terms and concepts:

<table border="2" cellpadding="5">
<colgroup>
    <col span="1" style="width: 20%;">
    <col span="1" style="width: 80%;">
</colgroup>
<tr><th>Term</th><th>Explanation</th></tr>
<tr><td>Parallel Template</td><td> A manifest for the construction of a parallel module. It is a set of
functions, which are used to define the various parallel resources
comprising the parallel module. The functions are designed in a monadic
style and always return a special parallel resource, the parallel function.
The corresponding type is `Framework.PTemplate`.</td></tr>
<tr><td>Entry Point</td><td>Special resource of type `Engine.Entry` returned from a parallel template, which is a function 
with first argument the program. This function is called at runtime to launch the calculations defined in the templated,
with all the compiled CUDA resources passed along through the program argument.</td></tr>
<tr><td>Parallel Resource</td><td> Various constructs to create a parallel module, such as kernels, textures,
parallel function, etc.</td></tr>
<tr><td>Resource Driver</td><td> The definition of parallel resources in a parallel template results in a set
of compile-time resources. They can be referenced from other compile-time
resources. An example is using a texture reference in a kernel code
quotation. To drive these resource at runtime, such as launching a
kernel, or binding a texture reference to device memory, you need to
apply a compiled CUDA module to it and get a driver for that resource.
We call this driver a resource driver. Most of the resource types have an
`Apply()` member, which is used to generate the runtime driver for that
resource.</td></tr>
<tr><td>Parallel Function</td><td> A special parallel resource, which is the return value of a parallel
template. It represents the interface between a parallel module and the
outside world. You can invoke it from the host and provides the logic to
use various parallel resources to do the computing with one or more
CUDA devices. Applied to a CUDA module, it becomes a driver, which is
simply an F# function. The corresponding type is `Framework.PFunc`.</td></tr>
<tr><td>Parallel Module</td><td> An executable module combining the compiled CUDA module and the
parallel function. It applies the CUDA module to the parallel function, so
that you can invoke like an ordinary F# function. The corresponding type
is `Framework.PModule`.</td></tr>
<tr><td>IR Module</td><td> A module of NVVM IR code. It is compiled from the parallel template.
The corresponding type is `Builder.IRModule`.</td></tr>
<tr><td>PTX Module</td><td> A module of PTX code. It is compiled from the IR module. The
corresponding type is `Builder.PTXModule`.</td></tr>
<tr><td>CUDA Module</td><td> A CUDA binary module. It is compiled from the PTX module. The
corresponding type is `Engine.Module`.</td></tr>
<tr><td>Module Building Context</td><td>The context for the compiler and linker, as determined by the compile and link options.</td></tr>
<tr><td>CUDA workflow</td><td>A computation expression used to create parallel templates.</td></tr>
</table>
