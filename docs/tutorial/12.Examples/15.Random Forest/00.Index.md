# Random Forest

This example is an implementation of the machine learning algorithm [random forest](http://en.wikipedia.org/wiki/Random_forest). It can be used for categorical or continuous features. For simplicity the presented implementation is for the latter case only. If you are interested in categorical features you might want read [Mathias’ blog](http://www.clear-lines.com/blog/post/Random-Forest-classification-in-F-first-cut.aspx).

Random forests were developed by Leo Breiman and Adele Cutler, their dominance in the field of machine learning has been illustrated nicely in a blog post at [Strata 2012](http://strataconf.com/strata2012/public/schedule/detail/22658): "Ensembles of decision trees (often known as random forests) have been the most successful general-purpose algorithm in modern times" when it comes to “automatically identify the structure, interactions, and relationships in the data”. Furthermore it is noted that "most [Kaggle](https://www.kaggle.com/) competitions have at least one top entry that heavily uses this approach". Random forests also have been the algorithm of choice for the [body part recognition](http://research.microsoft.com/pubs/145347/BodyPartRecognition.pdf) of Microsoft’s Kinect, a motion sensing input devices for Xbox consoles and Windows PCs. More details can be found in the paper [Why did Microsoft decide to use Random Forests in the Kinect](http://www.quora.com/Why-did-Microsoft-decide-to-use-Random-Forests-in-the-Kinect).

Random forests consist of an ensemble of decision trees. We therefore start to look at decision trees. A [decision tree](http://en.wikipedia.org/wiki/Decision_tree) is a [tree like graph](https://en.wikipedia.org/wiki/Tree_%28graph_theory%29) where on every node a decision, based on one single feature is made. Given a set of features, the tree is traversed from node to node according to these decisions till a leaf is reached. The label of this leaf is the prediction for the given feature set. A simple decision tree could be used to decide what you need to take with you when leaving the house:

<img src="../../content/images/SimplisticDecisionTree.png" width="175" alt="Simplistic tree.">

This tree has been constructed by reason only, not involving any data. In machine learning however the trees are created such that they optimally predict a given sample of data (training data).

A decision tree predicting the Iris specie from the later introduced [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set) might look like:

<img src="../../content/images/IrisDataDecisionTreeDepth3.png" width="400" alt="Decision tree example.">

This tree does not classify all the examples correctly. We could change this by increasing the maximum tree depth. By doing so, the tree can predict the sample data 100% correctly, but only by learning the noise in the samples. In the most extreme case the algorithm is equivalent to a dictionary containing every sample. This is knows as over-fitting and leads to bad results when using it for out of sample predictions. In order to overcome over-fitting, we can train multiple decision trees, by introducing weights for the samples and only considering a random subset of the features for each split. The final decision of the random forest will be determined by a majority vote on the trees’ predictions. This technique is also known as [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating). It helps to decrease the variance (error from sensitivity to noise in the training set) without increasing the bias (error introduced from not enough flexibility of the model).

## Algorithm

We first explain, how to train decision trees. In a second part we look on what addition steps are needed in order to get from decision trees to random forests. Finally we give some overview on how the parallelization is done on CPU as well as on GPU.

### Building Decision Trees

To build good decision trees a discrimination measure is required. A common choice is to use the split information or entropy as described in [Efficient C4.5](http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=991727) as measure. It is definded as

$$\rm{Split}(T) = \sum_{i,b} \frac{T_{i,b}}{T} \log_2 \left( \frac{T_{i,b}}{T} \right),$$

where $T_{i,b}$ is the number of samples with label $i$ in branch $b$ after the split and $T$ is the total number of samples. Other possible metrics are [information gain](http://en.wikipedia.org/wiki/Information_gain_in_decision_trees), the Gini impurity and variance reduction.

The search for a decision tree which is a global optimum for the discrimination measure is known as a [NP-complete](https://en.wikipedia.org/wiki/NP-complete) problem. We therefore focus on the a local optimization where for each split the local optimum is chosen, i.e. for every node we check which feature to use for splitting and where to split to get the lowest split information.

The steps to build a decision tree are:

1. For all features (e.g. for [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set): sepal-length, sepal-width, petal-length, petal-width), take all samples and order them by value.
2. For every split possibility, i.e. two successive samples with different values, calculate the split entropy:
	- Make a histogram based on labels for both branches after the split.
	- Calculate the split entropy as: $\rm{Split}(T)$
3. Choose the split for the feature and split possibility with the smallest split entropy.
4. Apply the algorithm from step 1 on all resulting sub-branches, until either all features have the same label, or the predefined maximal three depth has been reached.

We illustrate the calculation of the split entropy with a simple example. Assume we have ten samples for a single continuous feature, with three different labels all of them with weight one and the following value-label-map:

| feature value (sorted)  | label |
| ------------------------|-------|
|  1                      |  1    |
|  2                      |  2    |
|  3                      |  1    |
|  4                      |  1    |
|  5                      |  1    |
|  6                      |  1    |
|  7                      |  3    |
|  8                      |  3    |
|  9                      |  2    |
| 10                      |  3    |

We have nine split possibilities 1.5, 2.5, …, 9.5. The split between 1 and 2 at 1.5 would lead to the following two histograms. The lower branch would be

| label | counts |
|-------|--------|
| 1     | 5      |
| 2     | 1      |
| 3     | 0      |

whereas the upper branch would be

| label | counts |
|-------|--------|
| 1     | 0      |
| 2     | 1      |
| 3     | 3      |

The split at 1.5 leads to a split entropy of
$$\frac{5}{6} \log_2 \left( \frac{5}{6} \right) + \frac{1}{6} \log_2 \left( \frac{1}{6} \right) + \frac{1}{4} \log_2 \left( \frac{1}{4} \right) + \frac{3}{4} \log_2 \left( \frac{3}{4} \right) = 0.65 + 0.811 = 1.461.$$ 

We calculate the split entropies for all the other splits possibilities and plot them as a function of the split threshold:

<img src="../../content/images/Entropy.png" width="750" alt="Split entropies.">

We see from the plot that a split between 6 and 7 is ideal. The same procedure is now applied to the lower (samples 1-6) and the upper (samples 7-10) branch and the sub-branches thereof until either the maximum depth has been reached or the sub-branch only contains a single sample.

### Bagging Decision Trees to Random Forests

Decision trees, in particular those with many levels, suffer from over-fitting. To reduce the risk of over-fitting many different decision trees are trained and combined to an ensemble with majority vote. In order to get different trees, randomness is introduced at two points:

1. Every sample gets a weight which is randomly chosen for every tree. The weights are chosen by randomly selecting n samples out of the existing n samples with replacement. The histograms are then created not by counts, but by sums on these weights.
2. For every split only some randomly chosen features are considered.

To gain a deeper understanding the following [video of Jeremy Howard](https://www.youtube.com/watch?t=138&v=kwt6XEh7U3g) is helpful.

### GPU Parallelization
Only one, but heavily used method has been parallelized. It is called `Optimizer`, belongs to the type `IEntropyOptimizer` and returns for every feature the best splitting position and its entropy. On CPU `Array.parrallel.mapi` is used for parallelization, on the GPU several kernels are implemented and launched using two implementations: One with and one without CUDA-streams (in order to hide the data transfer behind GPU calculations, see also: [overlapping data transfer and GPU calculations using CUDA streams](http://devblogs.nvidia.com/parallelforall/how-overlap-data-transfers-cuda-cc/)).

## Overview on Code

- The file `DataModel.fs` contains the main data types.
- The type `Matrix` is defined in `Cublas.fs` it helps transferring matrices to GPU memory.
- `Array.fs` contains several functionality working on arrays.
- `GPUSplitEntropy.fs` contains all the functionality for the GPU implementation of `Optimizer`, as well as the function `entropyTerm` shared between CPU and GPU:
	- Code to calculate `minAndArgmin` using the `BlockReduce` algorithms from Alea CUDA.
	- Code to calculate cumulative sums using the `Alea.CUDA.Unbound` framework.
	- Type `EntropyOptimizationOptions` containing the parameters for the entropy optimization.
	- Type `EntropyOptimizationProblem` and `EntropyOptimizationMemories` to save entropy optimization related matrices.
	- Type `EntropyOptimizationModule` with the methods both called `Optimize` (with and without use of CUDA-streams).
- `RandomForest.fs` contains the CPU tree-building and evaluating algorithms as well as infrastructure code to integrate the GPU code:
	- Types for labels.
	- Evaluation function: `forecastTree` and `forecast`.
	- Several functions to calculate histograms & entropy needed in the function `optimizeFeatures` the CPU implementation of `Optimizer`.
	- The recursive function `trainTrees` training trees using a `sortedTrainingSet` as well as `weights` and an instance of `IEntropyOptimizer` with the method `Optimizer`.
	- Infrastructure code such as the type `EntropyDevice` creating an instance of `IEntropyOptimizer`.
	- The end-user functions `randomForestClassifier` and `randomStumpsClassifier` for training a random forest.
- `Performance.fs` contains a performance test of the CPU & GPU implementations. It also provides a Python script for a similar test.
- `IrisExample.fs` contains an example using the well-known Iris data set. As well as similar code for R and Python.

## Code:
