# Random Forest

This example is an implementation of the machine learning algorithm [Random Forest](http://en.wikipedia.org/wiki/Random_forest). It can be used for categorical or continuous features. The presented implementation is for the latter case.

Random Forests consist of an ensemble of `decision trees` combined by [bootstrap-aggregation (bagging)](http://en.wikipedia.org/wiki/Bootstrap_aggregating). The resulting models might be less transparent than decision trees but have the advantage of being less susceptible to over-fitting.

## Algorithm

We first explain, how to create decision trees. In a second part we look on what needs to be done in addition to tree building in order to get a random forest. Finally we give some overview on how the parallelization is done on CPU as well as on GPU.

### Building decision trees

A [decision tree](http://en.wikipedia.org/wiki/Decision_tree) is a [tree like graph](https://en.wikipedia.org/wiki/Tree_%28graph_theory%29) where on every node a decision, based on one single feature is made. Given a set of features, the tree is traversed from node to node according to these decisions till a leaf is reached. The label of this leaf is the prediction for the given feature set.

<img src="../../content/images/IrisDataDecisionTreeDepth3.png" width="718" alt="Decision tree example.">

In order to build a decision tree, the "optimal" decisions for every node has to be found. As a measure of "optimal" we use the split information (or entropy) as described in [Efficient C4.5](http://dx.doi.org/10.1109/69.991727)

$\rm{Split}(T) = \sum_{i,b} \frac{T_{i,b}}{T} \log_2 \left( \frac{T_{i,b}}{T} \right),$

where $T_{i,b}$ is the number of samples with label $i$ in branch $b$ after the split and $T$ is the total number of samples. Other possible metrics are [information gain](http://en.wikipedia.org/wiki/Information_gain_in_decision_trees), the Gini impurity and variance reduction.

A global minimization of the split information is known to be [NP-complete](https://en.wikipedia.org/wiki/NP-complete) problem. We therefore focus on the a local optimization where for each split the local optimum is chosen, i.e. for every node we check what feature at what split has the lowest split information.

The algorithm for building a decision tree looks like:

1. For all features (e.g. for [Iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set): sepal-length, sepal-width, petal-length, petal-width), take all samples and order them by value.
2. For every split possibility, i.e. two successive samples with different values, calculate the split entropy:
	- Make a histogram based on labels for both branches after the split.
	- Calculate the split entropy as: $\rm{Split}(T) = \sum_{i,b} \frac{T_{i,b}}{T} \log_2 \left( \frac{T_{i,b}}{T} \right)$
3. Choose the split for the feature and split possibility with the smallest split entropy.
4. Apply the algorithm from 1. on both resulting branches, till either all features have the same label, or the predefined maximal three depth has been reached.

A simple example on how to calculate the split entropy if only one feature exists: assume we have ten samples for a continuous feature, with three different labels and the following value-label-map:

| feature value (sorted)  | label |
| ------------------------|-------|
|  1                      |  1    |
|  2                      |  2    |
|  3                      |  1    |
|  4                      |  1    |
|  5                      |  1    |
|  6                      |  1    |
|  7                      |  3    |
|  8                      |  3    |
|  9                      |  2    |
| 10                      |  3    |

From the nine split possibilities (1.5, 2.5, ..., 9.5) let us have a look at the split between 6 and 7. It would lead to the following two histograms: lower branch:

| label | counts |
|-------|--------|
| 1     | 5      |
| 2     | 1      |
| 3     | 0      |

and the upper branch:

| label | counts |
|-------|--------|
| 1     | 0      |
| 2     | 1      |
| 3     | 3      |

implying a split entropy of $\frac{5}{6} \log_2 \left( \frac{5}{6} \right) + \frac{1}{6} \log_2 \left( \frac{1}{6} \right) + \frac{1}{4} \log_2 \left( \frac{1}{4} \right) + \frac{3}{4} \log_2 \left( \frac{3}{4} \right) = 0.65 + 0.811 = 1.461$. Calculations for all other splits leads to the following split entropies:

<img src="../../content/images/Entropy.png" width="500" alt="Split entropies.">

implying a split between 6 and 7. The same procedure is now applied to the lower (samples 1-6) and the upper (samples 7-10) branch.

### Bagging decision trees to random forests

As decision trees (especially very deep ones) suffer from over-fitting the idea is to have many different decision trees and have a majority vote on their results. This decreases the variance of the decision trees without increasing the bias.

In order to get different trees, randomness is introduced at several points.

1. Every sample gets a weight which is randomly chosen for every tree. The weights are chosen by randomly selecting `n` samples from `n` samples with replacement.
The histograms are then created not by counts, but by sums on these weights.
2. For every split only some (randomly chosen) features are considered.

### GPU Parallelization
Only one, but heavily used method has been parallelized. It is called `Optimizer`, belongs to the type `IEntropyOptimizer` and returns for every feature the best splitting position and its entropy. On CPU `Array.parrallel.mapi` is used for parallelization, on the GPU several kernels are implemented and launched using two implementations: One with and one without CUDA-streams (in order to hide the data transfer behind GPU calculations, see also: [overlapping data transfer and GPU calculations using CUDA streams](http://devblogs.nvidia.com/parallelforall/how-overlap-data-transfers-cuda-cc/)).

## Overview on Code

- The file `DataModel.fs` contains the main data types.
- The type `Matrix` is defined in `Cublas.fs` it helps transferring matrices to GPU memory.
- `Array.fs` contains several functionality working on arrays.
- `GPUSplitEntropy.fs` contains all the functionality for the GPU implementation of `Optimizer`, as well as the function `entropyTerm` shared between CPU and GPU:
	- Code to calculate `minAndArgmin` using the `BlockReduce` algorithms from Alea CUDA.
	- Code to calculate cumulative sums using the `Alea.CUDA.Unbound` framework.
	- Type `EntropyOptimizationOptions` containing the parameters for the entropy optimization.
	- Records `EntropyOptimizationProblem` and `EntropyOptimizationMemories` to save entropy optimization related matrices.
	- Type `EntropyOptimizationModule` with the methods both called `Optimize` one of them using CUDA-streams one of them not.
- `RandomForest.fs` contains the CPU tree-building and evaluating algorithms as well as infrastructure code to integrate the GPU code:
	- Types for labels.
	- Evaluation function: `forecastTree` and `forecast`.
	- Several functions to calculate histograms & entropy needed in the function `optimizeFeatures` the CPU implementation of `Optimizer`.
	- The recursive function `trainTrees` training trees using a `sortedTrainingSet` as well as `weights` and an instance of `IEntropyOptimizer` with the method `Optimizer`.
	- Infrastructure code such as the type `EntropyDevice` creating an instance of `IEntropyOptimizer`.
	- The end-user functions `randomForestClassifier` and `randomStumpsClassifier` for training a random forest.
- `Performance.fs` contains a performance test of the CPU & GPU implementations. It also provides a Python script for a similar test.
- `IrisExample.fs` contains an example using the well-known Iris data set. As well as similar code for R and Python.

## Code:
