# Random Forest

This example is an implementation of the machine learning method [Random Forest](http://en.wikipedia.org/wiki/Random_forest). It can be used for categorical or continuous features. The presented implementation is for the latter case.

Random Forests consist of an ensemble of decision trees combined by [bootstrap-aggregation (bagging)](http://en.wikipedia.org/wiki/Bootstrap_aggregating). The resulting models might be less transparent than decision trees but have the advantage of being less susceptible to over-fitting.

## Algorithm

We first explain, how to create decision trees. In a second part we look on what needs to be done in addition to tree building in order to get a random forest. Finally we give some overview on how the parallelization is done on CPU as well as on GPU.

### Building decision trees

A [decision tree](http://en.wikipedia.org/wiki/Decision_tree) is a tree like graph where on every node a decision, based on one single feature is made. Given a set of features, the tree is traversed from node to node according to these decisions till a leaf is reached. The label of this leaf is the prediction for the given feature set.

In order to build a decision tree, the "best" decisions for every node has to be found. As a measure of "best" we use the split information (or entropy) as described in [Efficient C4.5](http://dx.doi.org/10.1109/69.991727)

$\rm{Split}(T) = \sum_{i,b} \frac{T_{i,b}}{T} \log_2 \left( \frac{T_{i,b}}{T} \right)$

where $T_{i,b}$ is the number of samples with label $i$ in branch $b$ after the split and $T$ is the total number of samples. Other possible metricsâ€™ are ([information gain](http://en.wikipedia.org/wiki/Information_gain_in_decision_trees)), the Gini impurity or variance reduction.

The algorithm for building a decision tree looks like:

1. For all features (e.g. for Iris data set: sepal-length, sepal-width, petal-length, petal-width), take all samples and order them by value.
2. For every split possibility, i.e. two successive samples with different values, calculate the split entropy:
	- Make a histogram based on labels for both branches after the split.
	- Calculate the split entropy as: $\rm{Split}(T) = \sum_{i,b} \frac{T_{i,b}}{T} \log_2 \left( \frac{T_{i,b}}{T} \right)$
4. Choose the split for the feature and split possibility with the smallest split entropy.
5. Apply the algorithm from 1. on both resulting branches, till either all features have the same label, or the predefined maximal three depth has been reached.

A simple example on how to calculate the split entropy if only one feature exists: assume we have ten samples for a continuous feature, with three different labels and the following value-label-map:

| feature value (sorted)  | label |
| ------------------------|-------|
|  1                      |  1    |
|  2                      |  2    |
|  3                      |  1    |
|  4                      |  1    |
|  5                      |  1    |
|  6                      |  1    |
|  7                      |  3    |
|  8                      |  3    |
|  9                      |  2    |
| 10                      |  3    |

From the nine split possibilities (1.5, 2.5, ..., 9.5) let us have a look at the split between 6 and 7. It would leed to the following two histograms: lower branch:

| label | counts |
|-------|--------|
| 1     | 5      |
| 2     | 1      |
| 3     | 0      |

and the upper branch:

| label | counts |
|-------|--------|
| 1     | 0      |
| 2     | 1      |
| 3     | 3      |

implying a split entropy of $\frac{5}{6} \log_2 \left( \frac{5}{6} \right) + \frac{1}{6} \log_2 \left( \frac{1}{6} \right) + \frac{1}{4} \log_2 \left( \frac{1}{4} \right) + \frac{3}{4} \log_2 \left( \frac{3}{4} \right) = 0.65 + 0.811 = 1.461$. Calculations for all other splits leads to the following split entropies:

<img src="../../content/images/Entropy.png" width="500" alt="Split entropies.">

implying a split between 6 and 7. The same procedure is now applied to the lower (samples 1-6) and the upper (samples 7-10) branch.

### Bagging decision trees to random forests

As decision trees (especially very deep ones) suffer from over-fitting the idea is to have many different decision trees and have a majority vote on their results. This decreases the variance of decision trees without increasing the bias.

In order to get different trees, randomness is introduced at several points.

1. Every sample gets a weight which is randomly chosen for every tree. The weights are chosen by randomly selecting `n` samples from `n` samples with replacement.
The histograms are then created not by counts, but by sums on these weights.
2. For every split only some (randomly chosen) features are considered.

### GPU parallelization
Only one, but heavily used method has been parallelized. It is calles `optimizer`, belongs to the type `IEntropyOptimizer` and returns for every feature the best splitting position and its entropy. On CPU `Array.parrallel.mapi` is used, on GPU several kernels are implemented, and launched using two implementations. One with and one without CUDA-streams in order to hide the data transfer behind GPU calculations, see also: [overlapping data transfer and GPU calculations using CUDA streams](http://devblogs.nvidia.com/parallelforall/how-overlap-data-transfers-cuda-cc/).

## Overview on Code

- The file `DataModel.fs` contains the main data types.
- A matrix type `Matrix` is defined in `Cublas.fs` it helps transferring matrices to GPU memory.
- `Array.fs` contains several functionality working on arrays.

The random forest and decision tree building is implemented in the files `GPUSplitEntropy.fs` (GPU part) and `RandomForest.fs` (CPU implementation).

The main random forest functionality is implemented in `GPUSplitEntropy.fs` (mainly GPU implementation of method ) and `RandomForest.fs` (CPU implementation)

The main function for tree building is the recursive function `trainTrees` which takes an `IEntropyOptimizer` containing a function `Optimize` calculating an array of optimal split indices and split entropies for all features using given weights.

The GPU implementation for `IEntropyOptimizer` as well as the GPU specific infrastructure code is implemented in the `GpuSplitEntropy.fs` file. The CPU part as well as all the non GPU specific infrastructure code can be found in the `RandomForest.fs` file.

- `Performance.fs` contains a performance test of the CPU & GPU implementations. It also provides a Pyhton script for a similar test.
- `IrisExample.fs` contains an example using the well-known Iris data set. As well as similar code for R and Python.

## Code:
