# Random Forest
This example is an implementation of the machine learning method 'Random Forest'. It can be used for categorical or continuous features.
The presented algorithm is for the latter case. 

Random Forests consist of a ensemble of decision trees combined by [bootstrap-aggregation(bagging)](http://en.wikipedia.org/wiki/Bootstrap_aggregating). The
resulting models are less transparent than decision trees but have the advantage of being less susceptible to over-fitting.

## Algorithm

### Building decision trees
For every feature the sample data is sorted and the entropy for all possible splits is calculated. The feature with the split with lowest entropy $- \sum_i p_i \log_2(p_i)$ 
is used . The sample data is then split accordingly into a low and a high branch and the next iteration is started on both of these branches with all features still in use.

Example on using the Iris-dataset:


### Bagging decision trees
In order to bring randomness into the decision-tree building, every sample gets a random weight. Further randomness is introduced, by only allowing a randomly selected subset
for every tree level.

The result from different trees is choosen by majority voting.

### GPU parallelization strategy
