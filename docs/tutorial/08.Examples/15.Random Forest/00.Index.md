# Random Forest
This example is an implementation of the machine learning method `Random Forest`. It can be used for categorical or continuous features.
The presented implementation is for the latter case.

Random Forests consist of an ensemble of decision trees combined by [bootstrap-aggregation(bagging)](http://en.wikipedia.org/wiki/Bootstrap_aggregating). 
The resulting models might be less transparent than decision trees but have the advantage of being less susceptible to over-fitting.

## Algorithm

### Building decision trees

Calculate split entropy for all features (e.g. for Iris data set: sepal-length, sepal-width, petal-length, petal-width): so for the sepal-length, take all features and order them by value.
for every split possibility calculate the entropy, i.e.: $- \sum_i p_i \log_2(p_i)$ where $i$ runs over the different labels, e.g:

<img src="../../content/images/histogramOnLabels.png" width="500" alt="Histogram for entropy.">

would result in an entropy of: $- \frac{1}{6} log(\frac{1}{6}) - \frac{1}{2} log(\frac{1}{2}) - \frac{1}{3} log(\frac{1}{3}) \approx 1.459$.
Showing the entropy for all possible splits results in:

<img src="../../content/images/Entropy.png" width="500" alt="Entropy for different splits in sepal-length feature.">

As split index 12 has the lowest entropy, remember this index and its entropy.
Do the same for sepal-width, petal-length and petal-width. Make the split for the feature with lowest entropy. Group samples into two groups depending if they
are above or below the split. For both of those groups start again with this algorithm, till both groups have the same label or the maximal tree-depth has been reached.

### Bagging decision trees to random forests
In order to bring randomness into the decision-tree building, every sample gets a random weight. Further randomness is introduced, by only allowing a randomly selected subset
for every tree level.

The result from different trees is chosen by majority voting.

### GPU parallelization strategy
Preparing weights (i.e.expanding weights), creating the histogram, calculating the entropy and looking for the lowest entropy all can be done in parallel for the different
features. An additional version works with `Streams` and parallelizes in addition over the different trees.

