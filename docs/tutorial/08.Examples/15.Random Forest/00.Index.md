# Random Forest

This example is an implementation of the machine learning method [Random Forest](http://en.wikipedia.org/wiki/Random_forest). It can be used for categorical or continuous features. The presented implementation is for the latter case.

Random Forests consist of an ensemble of decision trees combined by [bootstrap-aggregation (bagging)](http://en.wikipedia.org/wiki/Bootstrap_aggregating).
The resulting models might be less transparent than decision trees but have the advantage of being less susceptible to over-fitting.

## Algorithm

We first explain, how to create decision trees. In a second part we look on what needs to be done in addition to tree building in order to get a random forest. Finally we give some overview on how the parallelization is done on CPU as well as on GPU.

### Building decision trees

A [decision tree](http://en.wikipedia.org/wiki/Decision_tree) is a tree like graph where on every node a decision, based on one single feature is made. Given a set of features, the tree is traversed from node to node according to these decisions till a leaf is reached. The label of this leaf is the prediction for the given feature set.

In order to build a decision tree, the "best" decisions for every node has to be found. As a measure of "best" we use the split information as described in [Efficient C4.5](http://dx.doi.org/10.1109/69.991727)

 $\rm{Split}(T) = \sum_{i,b} \frac{T_{i,b}}{T} \log_2(\frac{T_{i,b}}{T})$

where $T_{i,b}$ is the number of samples with label $i$ in branch $b$ after the split and $T$ is the total number of samples. Other possible metricsâ€™ are ([information gain](http://en.wikipedia.org/wiki/Information_gain_in_decision_trees)), the Gini impurity or variance reduction.

The algorithm for building a decision tree looks like:

Calculate the split entropy for all features (e.g. for Iris data set: sepal-length, sepal-width, petal-length, petal-width), take all samples and order them by value.
for every split possibility, i.e. two successive samples with different values, calculate the split entropy:

Make a histogram based on labels for both branches after split. For both of these branches $b \in \{1,2\}$ calculate the split entropy as:

$\rm{Split}(T) = \sum_{i,b} \frac{T_{i,b}}{T} \log_2(\frac{T_{i,b}}{T})$,

After the split, the same algorithm is applied again on both branches, till either all features have the same label, or the predefined maximal three depth has been reached.

### Bagging decision trees to random forests

As decision trees suffer from over fitting (especially very deep ones) the idea is to have many different decision trees and have a majority voting on their results. This decreases the variance of decision trees without increasing the [bias](http://en.wikipedia.org/wiki/Bias_of_an_estimator).

In order to get different trees, randomness is introduced at several points.

First every sample gets a weight which is randomly chosen for every tree. The weights are chosen by randomly selecting `n` samples from `n` samples with replacement.
The histograms are then not created by counts, but sums on these weights.

As a second source of randomness, for every split only some (randomly chosen) features are considered.

The result from different trees is selected by majority voting.

### GPU parallelization
Creation of weights is currently not parallelized, but could be done on GPU as well.

The `optimizer` method which returns for every feature the best splitting position and its entropy are parallelized. On CPU the method `Array.parrallel.mapi` instead of `Array.mapi` is used.
For the GPU implementation several different kernels were written. The kernel launch time has been reduced by initializing them lazily. A second implementation using CUDA-streams has been done (using the same kernels as the normal GPU implementation but different launching functionality) in order to hide the data transfer behind GPU calculations, see also: [overlapping data transfer and GPU calculations using CUDA streams](http://devblogs.nvidia.com/parallelforall/how-overlap-data-transfers-cuda-cc/).

## Overview on Code

- The file `DataModel.fs` contains the main data types.
- A matrix type `Matrix` is defined in `Cublas.fs` it helps transferring matrices to GPU memory.
- `Array.fs` contains several functionality working on arrays.

The main random forest functionality is implemented in `GPUSplitEntropy.fs` (mainly GPU implementation) and `RandomForest.fs` (CPU implementation)

The main function for tree building is the recursive function `trainTrees` which takes an `IEntropyOptimizer` containing a function `Optimize` calculating an array of optimal split indices and split entropies for all features for given weights.

The GPU implementation for `IEntropyOptimizer` as well as the GPU specific infrastructure code is implemented in the `GpuSplitEntropy.fs` file. The CPU part as well as all the non GPU specific infrastructure code can be found in the `RandomForest.fs` file.

- `Performance.fs` contains a performance test of the CPU & GPU implementations. It also provides a Pyhton script for a similar test in python.
- `IrisExample.fs` contains an example using the well-known Iris data set. As well as similar code for R and Python.

## Code:
