# cuDNN

Before starting with this tutorial you will need to download the CUDNN library [here](https://developer.nvidia.com/cuDNN).
The code below is a port of the C++ MNist sample which accompanies the [library](https://developer.nvidia.com/cuDNN).

Note that your project implementing cudnn needs to have "Prefer 32 bit" unchecked in the build properties.  Your assembly build configuration should be "Any CPU" and your program should be run as a 64bit process.  For the time being, NVidia does not provide a 32bit version of cudnn.  If you look at the files included with the cudnn library download, you will only see a cudnn64_65.dll.  If a 32bit version is ever provided, e.g. cudnn32_65.dll, you will not need to worry about any of these steps; AleaGPU will reference the dll according to your build configuration.

The example demonstrates how to use the CUDNN library to implement forward pass. The sample loads weights and biases from a trained network, takes a few images of digits and recognizes them.  The network was trained on the MNIST dataset using [Caffe](http://caffe.berkeleyvision.org/).  The network consists of two convolution layers, two pooling layers, one relu and two fully connected layers.  The final layer gets processed by Softmax.  cublasSgemv is used to implement the fully connected layers.

<table align="center" cellspacing="5" >
    <tr>
        <th>First Image</th>
        <th>Second Image</th>
        <th>Third Image</th></tr>
    <tr>
        <td><img src="../../content/images/one_28x28.jpg"></td>
        <td><img src="../../content/images/three_28x28.jpg"></td>
        <td><img src="../../content/images/five_28x28.jpg"></td>
    </tr>
</table>
