# CUDA Specific Terminologies

The CUDA programming model introduces the following concepts and terminologies: 

<table border="2" cellpadding="5">
<colgroup>
    <col span="1" style="width: 20%;">
    <col span="1" style="width: 80%;">
</colgroup>
<tr><th>Term</th><th>Explanation</th></tr>
<tr><td>Host</td><td> The host machine of a GPU device.</td></tr>
<tr><td>Device</td><td> A CUDA enabled GPU device.</td></tr>
<tr><td>Kernel</td><td> A function executing serial code in parallel on the device. A kernel runs
as a single instruction multiple thread (SIMT) computation, executed in
parallel by a set of threads.</td></tr>
<tr><td>Thread</td><td> A process on the device executing a kernel.</td></tr>
<tr><td>Thread Block</td><td> A set of threads which share a multiprocessor, its shared memory space,
and thread synchronization primitives.</td></tr>
<tr><td>Warp</td><td> A group of 16 or 32 threads executing synchronously within a block.</td></tr>
<tr><td>Grid</td><td> A set of blocks executing a single kernel.</td></tr>
<tr><td>Device Memory, or
Global Memory</td><td>
Off chip memory that is accessible to all threads and to the host.</td></tr>
<tr><td>Coalesced Memory Access</td><td>
Multiple global memory accesses that are conglomerated by the device
into a single memory transaction. Requires appropriate memory access
alignment and contiguity.</td></tr>
<tr><td>Streaming Multiprocessor (SM)</td><td>
Multiprocessor with an array of scalar processors, shared memory,
registers, cache for texture and constant memory. Thread blocks are
scheduled to be executed on SMs.</td></tr>
<tr><td>Scalar Processor (SP)</td><td> The scalar processors in an SM. A SP executes a single thread.</td></tr>
<tr><td>Occupancy</td><td> A measure of effectiveness for a kernel, defined as the ratio active
warps per SM / maximal warps per SM.</td></tr>
</table>